\documentclass[12pt]{report}
\input{../preamble/preamble}
\addbibresource{../bibliography.bib}

\usepackage{xr}
\externaldocument[xr-]{../main}
\newcommand{\xrprefix}[1]{xr-#1}

\begin{document}

\chapter*{Notation}
\addcontentsline{toc}{chapter}{Notation}  % Add an entry to the ToC.

\newcommand{\entry}[2]{
    \begin{center}
        \begin{tabularx}{\linewidth}{@{}p{2.5cm}L@{}}
            #1 & #2
        \end{tabularx}
    \end{center}
}
\newcommand{\eatspace}{\vspace{-\baselineskip}}

\vspace{-5pt}  % Make align perfectly with other pages.
\entry
    {$x \coloneqq y$}
    {$x$ is defined as $y$}
\entry
    {$x \eqqcolon y$}
    {$y$ is defined as $x$}

\section*{Scalars, Vectors, and Matrices}
\entry
    {$x$}
    {Scalar}
\entry
    {$x \land y$}
    {$\min(x, y)$}
\entry
    {$x \lor y$}
    {$\max(x, y)$}
\entry
    {$\vx$}
    {Vector}
\entry
    {$\vx_{i:j}$}
    {Subvector of $\vx$ consisting of elements $i$ through $j$}
\entry
    {$\vx_{A}$}
    {Subvector of $\vx$: $(x_i)_{i \in A}$}
\entry
    {$\vx \oplus \vy$}
    {Concatenation of $\vx$ and $\vy$}
\entry
    {$\abs{\vx}$}
    {Dimensionality of vector}
\entry
    {$\vnull$}
    {Vector of zeros}
\entry
    {$\mX$}
    {Matrix}
\entry
    {$\mI$}
    {Identity matrix}

\section*{Sets and Functions}
\entry
    {$A \subsetneq B$}
    {$A \sub B$, but $A \neq B$}
\entry
    {$A/\!\sim$}
    {All equivalence classes for an equivalence relation $\sim$ on $A$}
\entry
    {$x \mapsto \ldots$}
    {Function of $x$ without name; an anonymous function}
\entry
    {$f=x\mapsto g(x)$}
    {$f$ is equal to the function $g$}
\entry
    {$B^A$}
    {All functions $A \to B$}
\entry
    {$C(\X, \Y)$}
    {All continuous functions $\X \to \Y$}
\entry
    {$C_b(\X, \Y)$}
    {All continuous and bounded functions $\X \to \Y$}
\entry
    {$f|_A$}
    {Restriction of $f$ to the domain $A$}
\entry
    {$f \comp g$}
    {Composition of $f$ and $g$}
\entry
    {$f(\vx)$}
    {If $f\colon \R \to \R$, shorthand for $(f(x_1),\ldots, f(x_n))$}
\entry
    {$k(\vx, \vy)$}
    {If $k\colon \R \times \R \to \R$, shorthand for
    \begin{equation*}
        \begin{bmatrix}
            k(x_1, y_1) & \cdots & k(x_1, y_m) \\
            \vdots & \ddots & \vdots \\
            k(x_n, y_1) & \cdots & k(x_n, y_m)
        \end{bmatrix}
    \end{equation*}\eatspace}

\section*{Topology and Analysis}
\entry
    {$\to$}
    {Convergence}
\entry
    {$\weakto$}
    {Convergence in the weak topology}
\entry
    {$d$}
    {Metric}
\entry
    {$d_X$}
    {Metric on the space $X$}
\entry
    {$\norm{\vardot}$}
    {Norm}
\entry
    {$\norm{\vardot}_p$}
    {$p$-norm; for example, $\norm{\vardot}_2$ is the Euclidean norm}
\entry
    {$\norm{\vardot}_\Hb$}
    {Norm on the space $\Hb$}
\entry
    {$\lra{\vardot,\vardot}$}
    {Inner product}
\entry
    {$\lra{\vardot,\vardot}_\Hb$}
    {Inner product on the space $\Hb$}

\section*{Probability}
\entry
    {$p$}
    {Probability density}
\entry
    {$\P$}
    {Probability measure}
\entry
    {$\E$}
    {Expectation}
\entry
    {$\E_X$}
    {Expectation with respect to the random variable $X$}
\entry
    {$\E_p$}
    {Expectation with respect to the density $p$}
\entry
    {$\E_\mu$}
    {Expectation with respect to the probability measure $\mu$}
% \entry
%     {$\V$}
%     {Variance}
\entry
    {$\cov(X, Y)$}
    {Covariance between random variables $X$ and $Y$}
\entry
    {$\var(X)$}
    {Variance of random variable $X$}
\entry
    {$\norm{\vardot}_{L^p}$}
    {$L^p$ norm: $\E[(\vardot)^p]^{\frac1p}$}
\entry
    {$\norm{\vardot}_{L^p(\mu)}$}
    {$L^p$ norm with respect to $\mu$: $\E_\mu[(\vardot)^p]^{\frac1p}$}
\entry
    {$\KL(\mu, \nu)$}
    {Kullback--Leibler divergence of $\mu$ with respect to $\nu$}
\entry
    {$\Hb(\mu)$}
    {Differential entropy of $\mu$ with respect to the Lebesgue measure}
\entry
    {$\Normal(\vmu, \mSigma)$}
    {Gaussian distribution with mean vector $\vmu$ and covariance matrix $\mSigma$}
\entry
    {$\Ber(p)$}
    {Bernoulli distribution with probability $p$}
\entry
    {$o_\P(1)$}
    {A random variable that converges to zero in probability}

\section*{Measure Theory}
\entry
    {$B$}
    {A typical Borel set}
\entry
    {$\B(X)$}
    {Borel $\sigma$-algebra on $X$}
\entry
    {$\sigma(\G)$}
    {$\sigma$-algebra generated by $\G$}
\entry
    {$f \in \F$}
    {$f$ is measurable with respect to $\F$}
\entry
    {$T(\mu)$}
    {Pushforward measure: $T(\mu)(B) = \mu(T^{-1}(B))$}

\section*{Coordinate Projections}
\entry
    {$P_\vx$}
    {Projection onto coordinates $\vx$: $f \mapsto (f({x_1}), \ldots, f(x_n))$}
\entry
    {$P_\vx \mu$}
    {Law of $(f(x_1), \ldots, f(x_n))$, where $f \sim \mu$}
\entry
    {$P^\sigma_\vx \mu$}
    {
        Law of $(f(x_1) + \e_1, \ldots, f(x_n) + \e_n)$,
        where $f \sim \mu$ and $(\e_i)_{i=1}^n \overset{\mathclap{\text{i.i.d.}}}{\sim} \Normal(0, \sigma^2)$
    }

\section*{Miscellaneous}
\entry
    {$\Sb^n$}
    {All permutations of size $n$}
\entry
    {$\sigma$}
    {A typical permutation}
\entry
    {$\T_\tau$}
    {Translation by $\tau$ (\cref{\xrprefix{def:translation}})}

\section*{Meta-Learning and Neural Processes}
\entry
    {$\vx\us{(c)}$}
    {Context inputs}
\entry
    {$\vy\us{(c)}$}
    {Context outputs}
\entry
    {$D\us{(c)}$}
    {Context set}
\entry
    {$\vx\us{(t)}$}
    {Target inputs}
\entry
    {$\vy\us{(t)}$}
    {Target outputs}
\entry
    {$D\us{(t)}$}
    {Target set}
\entry
    {$q$}
    {Density of a prediction by a neural process}
\entry
    {$\Qc$}
    {Variational family}
\entry
    {$\L_M$}
    {Empirical neural process objective (\cref{\xrprefix{def:empirical_neural_process_objective}})}
\entry
    {$\L\ss{NP}$}
    {Infinite-sample neural process objective (\cref{\xrprefix{def:neural_process_objective}})}
\entry
    {$\enc$}
    {Encoder}
\entry
    {$\dec$}
    {Decoder}


\section*{Inputs, Outputs, and Data Sets}
\entry
    {$\X$}
    {Space of inputs}
\entry
    {$\Y$}
    {Space of outputs}
\entry
    {$I_N$}
    {Collection of all $N$ inputs: $\X^N$}
\entry
    {$I$}
    {Collection of all finite collections of inputs: $\union_{n \ge 0} I_N$}
\entry
    {$\D_N$}
    {All data sets of size $N$: $(\X \times \Y)^N$}
\entry
    {$\D_{\le N}$}
    {All data sets up to size $N$: $\union_{n = 0}^N \D_n$}
\entry
    {$\D$}
    {All data sets: $\union_{N \ge 0} \D_N$}
\entry
    {$D$}
    {A typical data set}
\entry
    {$[D]$}
    {Equivalence class of $D$ (\cref{\xrprefix{sec:repr_theorems:functions}})}
% \entry
%     {$\abs{D}$}
%     {Number of data points in $D$}

\section*{Stochastic Processes and Prediction Maps}
\entry
    {$\Pc$}
    {All stochastic processes}
% \entry
%     {$\Pc\ss{c}$}
%     {Continuous processes}
% \entry
%     {$\Pc\ss{G}$}
%     {Gaussian processes}
\entry
    {$\pi$}
    {A typical prediction map $ \D \to \Pc$}
\entry
    {$\pi_f$}
    {Posterior prediction map (\cref{\xrprefix{def:posterior_prediction_map},\xrprefix{def:posterior_prediction_map_formal}})}
% \entry
%     {$\M$ and $\M\ss{c}$}
%     {Continuous prediction maps (\cref{\xrprefix{def:continuous_prediction_map}})}
% \entry
%     {$\overline{\M}$ and $\overline{\M}\ss{c}$}
%     {Noisy prediction maps (\cref{\xrprefix{def:noisy_prediction_maps}})}
\entry
    {$m_\pi$}
    {Mean map of a prediction map $\pi$ (\cref{\xrprefix{def:mean_map}})}
\entry
    {$k_\pi$}
    {Kernel map of a prediction map $\pi$ (\cref{\xrprefix{def:kernel_map}})}
\entry
    {$v_\pi$}
    {Variance map of a prediction map $\pi$ (\cref{\xrprefix{def:variance_map}})}
\entry
    {$\tD$}
    {Data sets of interest (\cref{\xrprefix{sec:predmap:prelims}})}
\entry
    {$\tI$}
    {Target inputs of interest (\cref{\xrprefix{sec:predmap:prelims}})}




\end{document}
