\documentclass[12pt, twoside]{report}
\input{../../preamble/preamble}
\addbibresource{../../bibliography.bib}

\usepackage{xr}
\externaldocument[xr-]{../../main}
\newcommand{\xrprefix}[1]{xr-#1}

\begin{document}

\chapter{Conclusion}
\label{chap:conclusion}

\looseness=-1
The contributions of this thesis are best interpreted as new tools in the neural process toolbox.
These new tools are certainly not meant to replace prior work, but rather to add to existing methods.
Every tool has a particular regime where it works well and where it is the right choice.
It is the art of the neural process practitioner to correctly identify which tools are best suited for a particular application.
To conclude this thesis, we recapitulate the tools put forward in this thesis and provide advice for how the practitioner may decide to use them.

\section{New Tools in the Neural Process Toolbox}

The primary tool that this thesis puts forward are convolutional deep sets (\cref{\xrprefix{thm:conv_deep_set},\xrprefix{thm:conv_deep_set_dte}}).
We used convolutional deep sets to construct convolutional neural processes (ConvNPs; \cref{\xrprefix{sec:convcnps:convcnps}}).
In problems where the data is roughly stationary, convolutional neural processes offer strong alternatives to existing neural processes.
The models constructed in \cref{\xrprefix{sec:convcnps:convcnps}}, however, are just a few of many.
The more powerful idea is that convolutional deep sets are a flexible neural network building block
that can be generally used in spatial, temporal or spatio--temporal data problems.
A great illustration of this idea is \cref{\xrprefix{sec:experiments:climate}},
where,
following the approach by \textcite{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for},
we used neural processes to perform statistical downscaling.
In the setup of the ConvCNP and ConvGNP, convolutional deep sets in combination with a pointwise MLP (\cref{\xrprefix{def:pointwise_MLP}}) provided a mechanism to combine coarse-grained ERA-Interim reanalysis variables with high-resolution elevation data \parencite{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for};
and in the multiscale architecture of the AR ConvCNP, a cascade of convolutional deep sets seamlessly stitched together convolutional neural networks operating at different resolutions.
Although convolutional deep sets are flexible and can work well, we emphasise that the technique is not a panacea.
The main limitation of convolutional deep sets are demanding computational requirements deriving from the discretisation (\cref{\xrprefix{proc:discretisation}}).
%
Since we first published the Convolutional Conditional Neural Process \parencite[ConvCNP;][]{Gordon:2020:Convolutional_Conditional_Neural_Processes}, the ConvCNP has been extended to symmetries other than translation equivariance \parencite{Kawano:2021:Group_Equivariant_Conditional_Neural_processes,Holderrieth:2021:Equivariant_Learning_of_Stochastic_Fields} and formed the basis for various models in a variety of applications
\parencite{
Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction,
Shyshya:2020:Neural_Models_for_Non-Uniformly_Samples,
Petersen:2021:GP-ConvCNP_Better_Generalization_for_Convolutional,
Wang:2021:Global_Convolutional_Neural_Processes,
Vaughan:2021:Multivariate_Climate_Downscaling_with_LNPs,
Pondaven:2022:Convolutional_Neural_Processes_for_Inpainting,
Vaughan:2022:Convolutional_Conditional_Neural_Processes_for}.

The second tool that this thesis proposes is the idea of directly parametrising the covariance between target outputs.
This gave rise to Gaussian neural processes (GNPs; \cref{\xrprefix{sec:convcnps:gnp}}).
GNP are a contender to latent-variable neural processes \parencite[LNPs;][]{Garnelo:2018:Neural_Processes}
by also modelling dependencies between target outputs.
In problems where the data is roughly Gaussian, GNPs are a promising new model class to explore.
Like with convolutional deep sets, the idea of directly parametrising the covariance between target outputs is useful beyond the models presented in \cref{\xrprefix{chap:convcnps}}.
For example, we used the technique to enable correlated samples in the climate downscaling experiment (\cref{\xrprefix{sec:experiments:climate}}), 
and we illustrated that a GNP can even be the encoder and/or decoder in a LNP (\cref{\xrprefix{sec:software:examples}}), potentially combining the benefits of both classes.
Besides the GNPs explored in this thesis,
many more approaches to parametrising the covariance are possible. 
Since we first published the Fully Convolutional Gaussian Neural Process \parencite[FullConvGNP;][]{Bruinsma:2021:The_Gaussian_Neural_Process,Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}, we used the FullConvGNP to meta-learn posterior distributions in PAC-Bayes bounds \parencite{Foong:2021:How_Tight_Can_PAC-Bayes_Be}.

The third and final tool presented by this thesis are autoregressive conditional neural processes (AR CNPs; \cref{\xrprefix{sec:convcnps:ar}}).
Although CNPs are the simplest of neural processes,
by deploying CNPs in an autoregressive fashion, CNPs have the capacity to compete with much more sophisticated approaches.
Most notably, across all experiments, Gaussian or non-Gaussian, the AR ConvCNP has consistently been amongst the best performing models.
AR CNPs equip the neural process framework with a new knob where modelling complexity and computational expense at training time can be traded for computational expense at test time.
AR CNPs have not yet been published.

\section{Advice for the Neural Process Practitioner}

The neural process framework offers a great variety of models.
To choose a model, one should first consider the general strengths and weakness of the various classes of neural processes.
Just like there is no free lunch,
every class of neural processes gives up something:
CNPs do not model dependencies;
GNPs only produce Gaussian predictions;
LNPs require approximations and consequently produce inferior uncertainty; 
and AR CNPs are no longer consistent and have high computational cost at test time.
See also
\cref{\xrprefix{tab:comparison_of_neural_process_approaches}}.
Is it the job of the neural process practitioner to decide which shortcoming is most acceptable.
Within a class of neural processes, choosing the right model depends on the application,
but the summary in \cref{\xrprefix{sec:experiments:conclusion}} could be used to guide your decision.

Rather than choosing an existing model, the neural process framework can also be used to tailor a solution to a particular data problem.
For example, rather than making the choice between convolutional deep sets and an attentive mechanism,
one may perfectly well combine the two.
In fact,
mixing and matching the building blocks of neural processes
is the best way to utilise the neural process framework!

To incorporate context data into a neural process architecture,
the three main generic approaches are
deep sets \parencite[\cref{\xrprefix{thm:deep_set}};][]{Zaheer:2017:Deep_Sets,Edwards:2017:Towards_a_Neural_Statistician,Garnelo:2018:Neural_Processes,Wagstaff:2019:On_the_Limitations_of_Representing},
an attentive mechanism \parencite{Bahdanau:2015:Neural_Machine_Translation_by_Jointly,Vaswani:2017:Attention_Is_All_You_Need,Kim:2019:Attentive_Neural_Processes},
or convolutional deep sets (\cref{\xrprefix{thm:conv_deep_set},\xrprefix{thm:conv_deep_set_dte}}).
We now give general considerations that may help deciding between the three.
Deep sets are the most general and computationally cheapest approach, but are not very parameter efficient and might yield models that underfit.
An attentive mechanism often improves over deep sets, generally yielding better performance.
Attentive mechanisms, however, are computationally more expensive
and do not construct a fixed-dimensional intermediate encoding like deep sets do. %, which makes them less flexible than deep sets.
If the data is stationary or can be rendered approximately stationary by including more features, then
convolutional deep sets can be used.
Convolutional deep sets are only suitable for one, two, or three-dimensional inputs, and the computational cost is determined by the discretisation (\cref{\xrprefix{proc:discretisation}}).
When convolutional deep sets are appropriate, they often yield superior performance.

To successfully deploy convolutional deep sets,
we have a few words of more specific advice.
To begin with, the discretisation must not be unnecessarily fine, because that incurs unnecessary computational expensive;
nor must it be too coarse, because that loses important detail of the data.
Our recommendation is to decide on the smallest length scale in the data that needs to be captured, and to make the interpoint spacing of the discretisation half or one-fourth of that length scale.
Second, 
although theory says that the length scale of the Gaussian kernels may be anything,
practice shows that it must be chosen right.
Our experience is that models learn more quickly if the length scales are sufficiently small, but not too small.
We advice to initialise the length scales of the Gaussian kernels to twice the interpoint spacing of the discretisation.
Smaller length scales might introduce visual artefacts,
and larger length scales might hamper performance.
%
Third, the receptive field of the convolutional neural network (CNN) should be controlled explicitly.
It should be set according to how far observations should influence predictions.
For a typical CNN, the receptive field of a CNN is determined by the number of layers and kernel sizes of the convolutional filters. 
Finally, for a given receptive field, the capacity of the CNN should to be tuned appropriately, and here one can use all tricks in the book.
A honourable mention is the U-Net architecture \parencite{Ronneberger:2015:U-Net_Convolutional_Networks_for_Biomedical}, which effectively achieves the large receptive fields at reasonable parameter counts.

\end{document}



